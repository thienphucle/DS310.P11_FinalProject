{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "MODEL_NAME = \"ura-hcmut/ura-llama-7b-r64\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "\n",
    "# Quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    token=ACCESS_TOKEN,\n",
    "    model_max_length=2048,\n",
    "    padding_side=\"left\",\n",
    "    truncation_side=\"left\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=ACCESS_TOKEN,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate prompt for fact-checking\n",
    "def generate_prompt(statement, context, evidence):    \n",
    "    return f\"\"\"\n",
    "        Kiểm tra tính chính xác của các tuyên bố dựa trên thông tin được cung cấp. \n",
    "        Đọc kỹ tuyên bố, ngữ cảnh và các bằng chứng, sau đó phân loại tuyên bố vào một trong ba nhãn:\n",
    "        - *Support*: Nếu tuyên bố được hỗ trợ bởi ngữ cảnh hoặc bằng chứng.\n",
    "        - *Refuted*: Nếu tuyên bố bị bác bỏ bởi ngữ cảnh hoặc bằng chứng.\n",
    "        - *N.E.I*: Nếu Không đủ thông tin để xác minh tính đúng/sai của tuyên bố.\n",
    "\n",
    "        ### Nhiệm vụ:\n",
    "        1. Đọc và hiểu rõ *tuyên bố*: \"{statement}\".\n",
    "        2. Đọc *ngữ cảnh* sau: \"{context}\".\n",
    "        3. Phân tích *các bằng chứng liên quan* sau đây:\n",
    "        {evidence if evidence else \"Không có bằng chứng được cung cấp.\"}\n",
    "        4. Dựa trên thông tin trên, phân loại tuyên bố vào một trong ba nhãn (*Support*, *Refuted*, hoặc *N.E.I*).\n",
    "        5. Trả lời chỉ với duy nhất một nhãn chính xác và không giải thích thêm.\n",
    "\n",
    "        ### Example Responses:\n",
    "        - <RESPONSE>: Support\n",
    "        - <RESPONSE>: Refuted\n",
    "        - <RESPONSE>: N.E.I\n",
    "    \"\"\"\n",
    "\n",
    "def fact_check(statement, context, evidence, tokenizer, model):\n",
    "    try:\n",
    "        # Generate the prompt\n",
    "        prompt = generate_prompt(statement, context, evidence)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Generate output from the model\n",
    "        outputs = model.generate(**inputs, max_new_tokens=8, do_sample=False)\n",
    "        \n",
    "        # Decode and clean the response\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        response_label = response.split(\"<RESPONSE>:\")[-1].strip()  # Extract label after <RESPONSE>:\n",
    "        print(f\"Kết quả: {response_label}\\n\")\n",
    "        \n",
    "        # Map the response label to numeric categories\n",
    "        if \"Support\" in response_label:\n",
    "            return \"Supports\"\n",
    "        elif \"Refuted\" in response_label:\n",
    "            return \"Refutes\"\n",
    "        elif \"N.E.I\" in response_label:\n",
    "            return \"Not_Enough_Information\"\n",
    "        else:\n",
    "            return 0  # Unexpected output\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during fact-checking: {e}\")\n",
    "        return -1  # Fallback for errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train = pd.read_csv('/kaggle/input/viwilkifc/train_final.csv')\n",
    "dev = pd.read_csv('/kaggle/input/viwilkifc/dev_final.csv')\n",
    "test = pd.read_csv('/kaggle/input/viwilkifc/test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "predicted_labels = []\n",
    "for index, row in test.iterrows():\n",
    "    statement = row[\"claim\"]\n",
    "    context = row[\"context\"]\n",
    "    evidence_list = row[\"evidence\"]\n",
    "    \n",
    "    print(f\"Đang kiểm tra tuyên bố {index} trong testset: {statement}\")\n",
    "    \n",
    "    try:\n",
    "        # Predict label\n",
    "        label = fact_check(statement, context, evidence_list, tokenizer, model)\n",
    "        predicted_labels.append(label)\n",
    "        print(f\"Predicted label: {label} (0: Supports, 1: Refutes, 2: Not_Enough_Info, -1: error)\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing statement {index}: {e}\")\n",
    "        predicted_labels.append(-1)\n",
    "\n",
    "# Thêm nhãn dự đoán vào DataFrame hiện tại\n",
    "test[\"predicted_label\"] = predicted_labels\n",
    "\n",
    "# Lưu kết quả vào file CSV\n",
    "output_file = f\"fact_check_results_test.csv\"\n",
    "test.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"Kết quả đã được lưu vào file '{output_file}'.\")\n",
    "\n",
    "# Lưu vào results dictionary\n",
    "results['Test'] = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Results\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def evaluate_results(results_df, class_labels=[\"Supports\", \"Refutes\", \"Not_Enough_Information\"]):\n",
    "    y_true = results_df[\"gold_label\"].tolist()\n",
    "    y_pred = results_df[\"predicted_label\"].tolist()\n",
    "    num_classes = len(class_labels)\n",
    "    \n",
    "    # Binarize labels for multi-class ROC and PR-AUC\n",
    "    y_true_bin = label_binarize(y_true, classes=range(num_classes))\n",
    "    y_pred_bin = label_binarize(y_pred, classes=range(num_classes))\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    f1_micro = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # ROC-AUC and PR-AUC calculations\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred, pos_label=1)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred, pos_label=1)\n",
    "    pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score (Micro): {f1_micro:.2f}\")\n",
    "    print(f\"F1 Score (Macro): {f1_macro:.2f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "    print(f\"PR-AUC: {pr_auc:.2f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve and AUC for each class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, label in enumerate(class_labels):\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"{label} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve and PR-AUC for each class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, label in enumerate(class_labels):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "        pr_auc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, label=f\"{label} (PR-AUC = {pr_auc:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Evaluate\n",
    "fact_check_results = pd.read_csv(\"/kaggle/working/fact_check_results_test.csv\")\n",
    "evaluate_results(fact_check_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
