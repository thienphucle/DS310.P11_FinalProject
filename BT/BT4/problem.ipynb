{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f20f1a-45e8-423f-ad31-4f3aff1442e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "from math import log\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "# Constants for the beginning of a sentence token and its index\n",
    "BOS_TOKEN, BOS_IDX = '<bos>', 0\n",
    "\n",
    "# Global variables for logging and gradients\n",
    "ITER_NUM = SUB_ITER_NUM = 0\n",
    "GRAD = None\n",
    "\n",
    "def read_corpus(filename):\n",
    "    \"\"\"\n",
    "    Reads a corpus from a file and returns the data as a list of (X, Y) pairs.\n",
    "    Each X is a list of tokens, and each Y is a list of labels.\n",
    "    \"\"\"\n",
    "    data, X, Y = [], [], []\n",
    "\n",
    "    # Read each line in the file\n",
    "    for words in [line.strip().split() for line in open(filename)]:\n",
    "        if words:\n",
    "            X.append(words[:-1])  # Tokens except the last one\n",
    "            Y.append(words[-1])   # The last token is the label\n",
    "        elif X:\n",
    "            data.append((X, Y))\n",
    "            X, Y = [], []\n",
    "    if X:\n",
    "        data.append((X, Y))\n",
    "\n",
    "    return data\n",
    "\n",
    "def extract_text_features(text_tokens, index):\n",
    "    \"\"\"\n",
    "    Extracts features from the text tokens at a given index.\n",
    "    Returns a list of feature strings.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    # Extract features from positions relative to the current index (-2 to +2)\n",
    "    for i in range(-2, 3):\n",
    "        idx = index + i\n",
    "        if 0 <= idx < len(text_tokens):\n",
    "            token = text_tokens[idx][0]\n",
    "            features.append(f'unigram[{i}]:' + token)\n",
    "            if token.isupper():\n",
    "                features.append(f'is_upper[{i}]')\n",
    "            if token.isdigit():\n",
    "                features.append(f'is_digit[{i}]')\n",
    "\n",
    "    return features\n",
    "\n",
    "class FeatureSet():\n",
    "    \"\"\"\n",
    "    Class to manage features and their mappings to feature IDs.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_dict = dict()\n",
    "        self.empirical_counts = Counter()\n",
    "        self.num_features = 0\n",
    "        self.label2index = {BOS_TOKEN: BOS_IDX}\n",
    "        self.label_array = [BOS_TOKEN]\n",
    "\n",
    "    def process_corpus(self, data):\n",
    "        \"\"\"\n",
    "        Processes the corpus to build the feature set and label mappings.\n",
    "        \"\"\"\n",
    "        for X, Y in data:\n",
    "            prev_y = BOS_IDX  # Start with the beginning of sentence index\n",
    "            for i in range(len(X)):\n",
    "                y = self.label2index.get(Y[i], len(self.label2index))\n",
    "                if Y[i] not in self.label2index:\n",
    "                    self.label2index[Y[i]] = y\n",
    "                    self.label_array.append(Y[i])\n",
    "                self._add(prev_y, y, X, i)\n",
    "                prev_y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_features\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the label mappings.\n",
    "        \"\"\"\n",
    "        return self.label2index, self.label_array\n",
    "\n",
    "    def _add(self, prev_y, y, X, i):\n",
    "        \"\"\"\n",
    "        Adds features to the feature set for a given position in the sentence.\n",
    "        \"\"\"\n",
    "        for feature_string in extract_text_features(X, i):\n",
    "            if feature_string not in self.feature_dict:\n",
    "                self.feature_dict[feature_string] = {}\n",
    "\n",
    "            for pair in [(prev_y, y), (-1, y)]:\n",
    "                if pair not in self.feature_dict[feature_string]:\n",
    "                    self.feature_dict[feature_string][pair] = self.num_features\n",
    "                    self.num_features += 1\n",
    "\n",
    "                feature_id = self.feature_dict[feature_string][pair]\n",
    "                self.empirical_counts[feature_id] += 1\n",
    "\n",
    "    def get_empirical_counts_numpy(self):\n",
    "        \"\"\"\n",
    "        Returns the empirical feature counts as a NumPy array.\n",
    "        \"\"\"\n",
    "        return np.array([self.empirical_counts.get(feature_id, 0) for feature_id in range(self.num_features)])\n",
    "\n",
    "    def get_empirical_counts_torch(self):\n",
    "        \"\"\"\n",
    "        Returns the empirical feature counts as a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        counts = torch.zeros(self.num_features)\n",
    "        for feature_id, count in self.empirical_counts.items():\n",
    "            counts[feature_id] = count\n",
    "        return counts\n",
    "\n",
    "    def get_feature_list(self, X, i):\n",
    "        \"\"\"\n",
    "        Returns the list of features and their IDs for a given position.\n",
    "        \"\"\"\n",
    "        feature_list_dict = defaultdict(set)\n",
    "        for feature_string in extract_text_features(X, i):\n",
    "            for (prev_y, y), feature_id in self.feature_dict[feature_string].items():\n",
    "                feature_list_dict[(prev_y, y)].add(feature_id)\n",
    "        return list(feature_list_dict.items())\n",
    "\n",
    "    def calc_inner_products_numpy(self, params, X, i):\n",
    "        \"\"\"\n",
    "        Calculates inner products for features at position i using NumPy arrays.\n",
    "        \"\"\"\n",
    "        inner_products = defaultdict(float)\n",
    "        features = chain.from_iterable(self.feature_dict.get(feature_string, {}).items() for feature_string in extract_text_features(X, i))\n",
    "\n",
    "        for (prev_y, y), feature_id in features:\n",
    "            inner_products[(prev_y, y)] += params[feature_id]\n",
    "\n",
    "        return [((prev_y, y), score) for (prev_y, y), score in inner_products.items()]\n",
    "\n",
    "    def calc_inner_products_torch(self, params, X, i):\n",
    "        \"\"\"\n",
    "        Calculates inner products for features at position i using PyTorch tensors.\n",
    "        \"\"\"\n",
    "        inner_products = defaultdict(float)\n",
    "        features = chain.from_iterable(\n",
    "            self.feature_dict.get(feature_string, {}).items()\n",
    "            for feature_string in extract_text_features(X, i)\n",
    "        )\n",
    "\n",
    "        for (prev_y, y), feature_id in features:\n",
    "            inner_products[(prev_y, y)] += params[feature_id]\n",
    "        return [((prev_y, y), score) for (prev_y, y), score in inner_products.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133d0e7c-4446-4b47-9c37-3a6680f36797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearChainCRFNumpy():\n",
    "    \"\"\"\n",
    "    Linear Chain CRF implemented using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.training_data = None\n",
    "        self.feature_set = None\n",
    "        self.label2index = None\n",
    "        self.label_array = None\n",
    "        self.num_classes = None\n",
    "        self.params = None\n",
    "        self.squared_sigma = 10.0  # Regularization parameter\n",
    "\n",
    "    def get_training_feature_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the training data by extracting features.\n",
    "        \"\"\"\n",
    "        return [[self.feature_set.get_feature_list(X, i) for i in range(len(X))]\n",
    "                for X, _ in self.training_data]\n",
    "\n",
    "    def estimate_parameters(self):\n",
    "        \"\"\"\n",
    "        Estimates the model parameters using L-BFGS optimization.\n",
    "        \"\"\"\n",
    "        def _callback(params):\n",
    "            global ITER_NUM, SUB_ITER_NUM\n",
    "            ITER_NUM += 1\n",
    "            SUB_ITER_NUM = 0\n",
    "\n",
    "        self.params, _, _ = fmin_l_bfgs_b(\n",
    "            func=self.negative_log_likelihood,\n",
    "            fprime=self.gradient,\n",
    "            x0=np.zeros(len(self.feature_set)),\n",
    "            args=(\n",
    "                self.feature_set,\n",
    "                self.get_training_feature_data(),\n",
    "                self.feature_set.get_empirical_counts_numpy(),\n",
    "                self.label2index,\n",
    "                self.squared_sigma\n",
    "            ),\n",
    "            maxiter=20,\n",
    "            callback=_callback\n",
    "        )\n",
    "\n",
    "    def calc_transition_matrices(self, params, num_classes, feature_set, X, inference=True):\n",
    "        \"\"\"\n",
    "        Calculates the transition matrices M for each position in the sequence.\n",
    "        \"\"\"\n",
    "        M = [np.zeros((num_classes, num_classes)) for _ in range(len(X))]\n",
    "        \n",
    "        for i, xi in enumerate(X):\n",
    "            # Get the feature scores for the current position\n",
    "            feature_pairs = feature_set.calc_inner_products_numpy(params, X, i) if inference else xi\n",
    "            for (prev_y, y), feature_scores in feature_pairs:\n",
    "                # Sum the feature scores for each (prev_y, y) pair\n",
    "                score = sum(params[fid] for fid in feature_scores) if not inference else feature_scores\n",
    "                indices = prev_y if prev_y != -1 else slice(None)\n",
    "                M[i][indices, y] += score\n",
    "\n",
    "            # Exponentiate the scores to get probabilities\n",
    "            M[i] = np.exp(M[i])\n",
    "            # Apply transition constraints\n",
    "            if i == 0:\n",
    "                M[i][BOS_IDX + 1:, :] = 0\n",
    "            else:\n",
    "                M[i][:, BOS_IDX] = 0\n",
    "                M[i][BOS_IDX, :] = 0\n",
    "        \n",
    "        return M\n",
    "\n",
    "    def forward_backward(self, num_classes, seq_length, M):\n",
    "        \"\"\"\n",
    "        Performs the forward-backward algorithm to compute alpha and beta.\n",
    "        \"\"\"\n",
    "        alpha = np.zeros((seq_length, num_classes))\n",
    "        beta = np.zeros((seq_length, num_classes))\n",
    "        \n",
    "        alpha[0, :] = M[0][BOS_IDX, :]  # Initial alpha values\n",
    "        for i in range(1, seq_length):\n",
    "            alpha[i] = np.dot(alpha[i - 1], M[i])\n",
    "\n",
    "        beta[-1, :] = 1.0  # Initial beta values\n",
    "        for i in range(seq_length - 2, -1, -1):\n",
    "            beta[i] = np.dot(M[i + 1], beta[i + 1])\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "    def negative_log_likelihood(self, params, feature_set, sequences, empirical_counts, label2index, squared_sigma):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood for optimization.\n",
    "        \"\"\"\n",
    "        expected_counts = np.zeros(len(feature_set))\n",
    "        log_Z = 0\n",
    "        \n",
    "        for X in sequences:\n",
    "            M = self.calc_transition_matrices(params, len(label2index), feature_set, X, inference=False)\n",
    "            alpha, beta = self.forward_backward(len(label2index), len(X), M)\n",
    "            Z = alpha[-1].sum()\n",
    "            log_Z += log(Z)\n",
    "\n",
    "            for i, xi in enumerate(X):\n",
    "                for (prev_y, y), feature_ids in xi:\n",
    "                    # Compute marginal probabilities\n",
    "                    if prev_y == -1:\n",
    "                        prob = alpha[i, y] * beta[i, y]\n",
    "                    elif i == 0 and prev_y == BOS_IDX:\n",
    "                        prob = M[i][BOS_IDX, y] * beta[i, y]\n",
    "                    else:\n",
    "                        prob = alpha[i - 1, prev_y] * M[i][prev_y, y] * beta[i, y]\n",
    "                    for fid in feature_ids:\n",
    "                        expected_counts[fid] += prob / Z\n",
    "\n",
    "        # Compute the regularized negative log-likelihood\n",
    "        log_likelihood = np.dot(empirical_counts, params) - log_Z - np.sum(params ** 2) / (2 * squared_sigma)\n",
    "\n",
    "        # Store the gradient for use in optimization\n",
    "        global GRAD, ITER_NUM, SUB_ITER_NUM\n",
    "        GRAD = empirical_counts - expected_counts - params / squared_sigma\n",
    "        print(f\"{ITER_NUM:03d} {(f'({SUB_ITER_NUM:02d})' if SUB_ITER_NUM else '')}: {-log_likelihood:.4f}\")\n",
    "        SUB_ITER_NUM += 1\n",
    "\n",
    "        return -log_likelihood\n",
    "\n",
    "    def gradient(self, params, *args):\n",
    "        \"\"\"\n",
    "        Returns the gradient for optimization.\n",
    "        \"\"\"\n",
    "        return -GRAD\n",
    "\n",
    "    def train(self, train_corpus_filename):\n",
    "        \"\"\"\n",
    "        Trains the model using the training corpus.\n",
    "        \"\"\"\n",
    "        global ITER_NUM, SUB_ITER_NUM\n",
    "        ITER_NUM = SUB_ITER_NUM = 0\n",
    "        self.training_data = read_corpus(train_corpus_filename)\n",
    "        self.feature_set = FeatureSet()\n",
    "        self.feature_set.process_corpus(self.training_data)\n",
    "        self.label2index, self.label_array = self.feature_set.get_labels()\n",
    "        self.index2label = {idx: label for label, idx in self.label2index.items()}\n",
    "        self.num_classes = len(self.label_array)\n",
    "        self.estimate_parameters()\n",
    "\n",
    "    def test(self, test_corpus_filename):\n",
    "        \"\"\"\n",
    "        Tests the model using the test corpus and reports accuracy.\n",
    "        \"\"\"\n",
    "        test_data = read_corpus(test_corpus_filename)\n",
    "        total_count, correct_count = 0, 0\n",
    "        for X, Y in test_data:\n",
    "            total_count += len(Y)\n",
    "            correct_count += sum(y == yp for y, yp in zip(Y, self.inference(X)))\n",
    "        print(f'LinearChainCRFNumpy Accuracy: {100 * correct_count / total_count:.2f}%')\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\"\n",
    "        Performs inference (Viterbi decoding) on a given sequence X.\n",
    "        \"\"\"\n",
    "        return self.viterbi(X, self.calc_transition_matrices(self.params, self.num_classes, self.feature_set, X, inference=True))\n",
    "\n",
    "    def viterbi(self, X, M):\n",
    "        \"\"\"\n",
    "        Viterbi decoding algorithm to find the most probable sequence of labels.\n",
    "        \"\"\"\n",
    "        seq_len = len(X)\n",
    "        V = np.zeros((seq_len, self.num_classes))\n",
    "        backptr = np.zeros((seq_len, self.num_classes), dtype='int64')\n",
    "        V[0, :] = M[0][BOS_IDX, :]  # Initialization\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            for y in range(self.num_classes):\n",
    "                probabilities = V[i - 1] * M[i][:, y]\n",
    "                backptr[i, y] = best_prev_y = np.argmax(probabilities)\n",
    "                V[i, y] = probabilities[best_prev_y]\n",
    "\n",
    "        last_y = np.argmax(V[-1])\n",
    "        Y_pred = [last_y]\n",
    "        \n",
    "        for i in range(seq_len - 1, 0, -1):\n",
    "            last_y = backptr[i, last_y]\n",
    "            Y_pred.append(last_y)\n",
    "\n",
    "        return [self.index2label[y] for y in reversed(Y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0027e968-bfa0-4d46-8942-97bb421fdc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearChainCRFPytorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Chain CRF implemented using PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(LinearChainCRFPytorch, self).__init__()\n",
    "        self.squared_sigma = 10.0  # Regularization parameter\n",
    "\n",
    "    def get_training_feature_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the training data by extracting features.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            [self.feature_set.get_feature_list(X, i) for i in range(len(X))]\n",
    "            for X, _ in self.training_data\n",
    "        ]\n",
    "\n",
    "    def estimate_parameters(self):\n",
    "        \"\"\"\n",
    "        Estimates the model parameters using PyTorch's LBFGS optimizer.\n",
    "        \"\"\"\n",
    "        empirical_counts = self.feature_set.get_empirical_counts_torch()\n",
    "        sequences = self.get_training_feature_data()\n",
    "\n",
    "        self.params = nn.Parameter(torch.zeros(len(self.feature_set)))\n",
    "\n",
    "        optimizer = torch.optim.LBFGS([self.params], max_iter=20)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.negative_log_likelihood(self.params, sequences, empirical_counts)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    def calc_transition_matrices(self, params, num_classes, feature_set, X, inference=True):\n",
    "        \"\"\"\n",
    "        Calculates the transition matrices M for each position in the sequence.\n",
    "        \"\"\"\n",
    "        M = []\n",
    "        \n",
    "        ### BEGIN SOLUTION        \n",
    "        for i in range(len(X)):\n",
    "            # Initialize the transition matrix with -inf (log-space equivalent of 0 probability)\n",
    "            M_i = torch.full((num_classes, num_classes), float('-inf'))\n",
    "            \n",
    "            # Compute feature pairs for the current position\n",
    "            feature_pairs = feature_set.calc_inner_products_torch(params, X, i) if inference else X[i]\n",
    "            \n",
    "            for (prev_y, y), scores in feature_pairs:\n",
    "                # Ensure scores are scalar (aggregate if needed)\n",
    "                if isinstance(scores, set):\n",
    "                    scores = sum(scores)  # Aggregate set into a single scalar (can also use mean or max)\n",
    "                elif not isinstance(scores, (float, torch.Tensor)):\n",
    "                    raise ValueError(f\"Unexpected score type: {type(scores)} in feature_pairs: {feature_pairs}\")\n",
    "                \n",
    "                if prev_y == -1:  # Unary features\n",
    "                    M_i[:, y] = torch.where(M_i[:, y] == float('-inf'), scores, M_i[:, y] + scores)\n",
    "                else:  # Pairwise features\n",
    "                    M_i[prev_y, y] = torch.where(M_i[prev_y, y] == float('-inf'), scores, M_i[prev_y, y] + scores)\n",
    "            \n",
    "            # Handle special BOS constraints\n",
    "            if i == 0:\n",
    "                M_i[BOS_IDX + 1:, :] = float('-inf')  # Disallow transitions from non-BOS states at the start\n",
    "            else:\n",
    "                M_i[:, BOS_IDX] = float('-inf')  # Disallow transitions to BOS\n",
    "                M_i[BOS_IDX, :] = float('-inf')  # Disallow transitions from BOS\n",
    "\n",
    "            M.append(M_i)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return M\n",
    "\n",
    "    def forward_backward(self, num_classes, seq_length, M):\n",
    "        \"\"\"\n",
    "        Performs the forward-backward algorithm in log-space using PyTorch tensors.\n",
    "        \"\"\"\n",
    "        alpha = []\n",
    "        beta = []\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        alpha = torch.full((seq_length, num_classes), float('-inf'))\n",
    "        beta = torch.full((seq_length, num_classes), float('-inf'))\n",
    "\n",
    "        # Forward pass\n",
    "        alpha[0, :] = M[0][BOS_IDX, :]\n",
    "        for i in range(1, seq_length):\n",
    "            alpha[i, :] = torch.logsumexp(alpha[i - 1].unsqueeze(1) + M[i], dim=0)\n",
    "\n",
    "        # Backward pass\n",
    "        beta[-1, :] = 0  # Log(1) = 0\n",
    "        for i in range(seq_length - 2, -1, -1):\n",
    "            beta[i, :] = torch.logsumexp(M[i + 1] + beta[i + 1].unsqueeze(0), dim=1)\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return alpha, beta\n",
    "\n",
    "    def negative_log_likelihood(self, params, sequences, empirical_counts):\n",
    "        \"\"\"\n",
    "        Computes the negative log-likelihood for optimization.\n",
    "        \"\"\"\n",
    "        expected_counts = torch.zeros_like(params)\n",
    "        total_log_Z = 0.0\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        for X in sequences:\n",
    "            M = self.calc_transition_matrices(params, self.num_classes, self.feature_set, X, inference=False)\n",
    "            alpha, beta = self.forward_backward(self.num_classes, len(X), M)\n",
    "            Z = torch.logsumexp(alpha[-1], dim=0)\n",
    "            total_log_Z += Z\n",
    "\n",
    "            # Compute expected counts using vectorized operations\n",
    "            for i, feature_pairs in enumerate(X):\n",
    "                alpha_broadcast = alpha[i - 1].unsqueeze(1) if i > 0 else torch.zeros(self.num_classes).unsqueeze(1)\n",
    "                beta_broadcast = beta[i].unsqueeze(0)\n",
    "\n",
    "                for (prev_y, y), feature_ids in feature_pairs:\n",
    "                    if prev_y == -1:  # Unary feature\n",
    "                        prob = alpha[i, y] + beta[i, y]\n",
    "                    else:  # Pairwise feature\n",
    "                        prob = alpha_broadcast[prev_y, 0] + M[i][prev_y, y] + beta_broadcast[0, y]\n",
    "\n",
    "                    # Update expected counts\n",
    "                    prob_normalized = torch.exp(prob - Z)\n",
    "                    for fid in feature_ids:\n",
    "                        expected_counts[fid] += prob_normalized\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        # Compute the regularized negative log-likelihood\n",
    "        global ITER_NUM\n",
    "        regularization = torch.sum(params ** 2) / (2 * self.squared_sigma)\n",
    "        log_likelihood = torch.dot(empirical_counts, params) - total_log_Z - regularization\n",
    "        loss = -log_likelihood\n",
    "        print(f'{ITER_NUM:03d} : {loss.item()}')\n",
    "        ITER_NUM += 1\n",
    "        return loss\n",
    "\n",
    "    def train(self, train_corpus_filename):\n",
    "        \"\"\"\n",
    "        Trains the model using the training corpus.\n",
    "        \"\"\"\n",
    "        global ITER_NUM\n",
    "        ITER_NUM = 0\n",
    "        self.training_data = read_corpus(train_corpus_filename)\n",
    "        self.feature_set = FeatureSet()\n",
    "        self.feature_set.process_corpus(self.training_data)\n",
    "        self.label2index, self.label_array = self.feature_set.get_labels()\n",
    "        self.index2label = {idx: label for label, idx in self.label2index.items()}\n",
    "        self.num_classes = len(self.label_array)\n",
    "        self.estimate_parameters()\n",
    "\n",
    "    def test(self, test_corpus_filename):\n",
    "        \"\"\"\n",
    "        Tests the model using the test corpus and reports accuracy.\n",
    "        \"\"\"\n",
    "        test_data = read_corpus(test_corpus_filename)\n",
    "        total_count, correct_count = 0, 0\n",
    "        for X, Y in test_data:\n",
    "            total_count += len(Y)\n",
    "            predicted_Y = self.inference(X)\n",
    "            correct_count += sum(y == yp for y, yp in zip(Y, predicted_Y))\n",
    "        print(f'LinearChainCRFPytorch Accuracy: {100 * correct_count / total_count:.2f}%')\n",
    "\n",
    "    def inference(self, X):\n",
    "        \"\"\"\n",
    "        Performs inference (Viterbi decoding) on a given sequence X.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        M = self.calc_transition_matrices(self.params, self.num_classes, self.feature_set, X, inference=True)\n",
    "        return self.viterbi(X, M)\n",
    "        ### END SOLUTION\n",
    "\n",
    "    def viterbi(self, M):\n",
    "        \"\"\"\n",
    "        Viterbi decoding algorithm to find the most probable sequence of labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN SOLUTION\n",
    "        seq_len = len(M)\n",
    "        V = torch.full((seq_len, self.num_classes), float('-inf'))\n",
    "        backptr = torch.zeros((seq_len, self.num_classes), dtype=torch.long)\n",
    "\n",
    "        V[0, :] = M[0][BOS_IDX, :]  # Initialization\n",
    "        for i in range(1, seq_len):\n",
    "            probabilities = V[i - 1].unsqueeze(1) + M[i]\n",
    "            backptr[i] = torch.argmax(probabilities, dim=0)\n",
    "            V[i] = torch.max(probabilities, dim=0).values\n",
    "\n",
    "        last_y = torch.argmax(V[-1]).item()\n",
    "        Y_pred = [last_y]\n",
    "\n",
    "        for i in range(seq_len - 1, 0, -1):\n",
    "            last_y = backptr[i, last_y].item()\n",
    "            Y_pred.append(last_y)\n",
    "\n",
    "        ### END SOLUTION\n",
    "        \n",
    "        return [self.index2label[y] for y in reversed(Y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79ee4f0-3405-4265-86ca-dd0cfc94d218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 : 5003.6527\n",
      "000 (01): 4668.3695\n",
      "000 (02): 3418.6408\n",
      "001 : 1281.3727\n",
      "002 : 603.3559\n",
      "003 : 304.4369\n",
      "004 : 199.3623\n",
      "005 : 174.8886\n",
      "006 : 150.4813\n",
      "007 : 142.9658\n",
      "008 : 135.2746\n",
      "009 : 129.5327\n",
      "010 : 126.7701\n",
      "011 : 123.8661\n",
      "012 : 121.4586\n",
      "013 : 117.1418\n",
      "014 : 115.5125\n",
      "015 : 114.3371\n",
      "016 : 113.9227\n",
      "017 : 113.5398\n",
      "018 : 113.2865\n",
      "019 : 113.1469\n",
      "LinearChainCRFNumpy Accuracy: 77.77%\n"
     ]
    }
   ],
   "source": [
    "# Train and test the NumPy model\n",
    "numpy_model = LinearChainCRFNumpy()\n",
    "numpy_model.train('data/train.txt')\n",
    "numpy_model.test('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4e1e0a4-e734-4f07-bb6b-5a5bfba6c429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000 : 80149800.0\n",
      "001 : 80149792.0\n",
      "002 : 79480368.0\n",
      "003 : 79480360.0\n",
      "004 : 79480360.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LinearChainCRFPytorch.viterbi() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m torch_model \u001b[38;5;241m=\u001b[39m LinearChainCRFPytorch()\n\u001b[0;32m      3\u001b[0m torch_model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtorch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/test.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 161\u001b[0m, in \u001b[0;36mLinearChainCRFPytorch.test\u001b[1;34m(self, test_corpus_filename)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, Y \u001b[38;5;129;01min\u001b[39;00m test_data:\n\u001b[0;32m    160\u001b[0m     total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y)\n\u001b[1;32m--> 161\u001b[0m     predicted_Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     correct_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(y \u001b[38;5;241m==\u001b[39m yp \u001b[38;5;28;01mfor\u001b[39;00m y, yp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(Y, predicted_Y))\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinearChainCRFPytorch Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mcorrect_count\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal_count\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 172\u001b[0m, in \u001b[0;36mLinearChainCRFPytorch.inference\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m### BEGIN SOLUTION\u001b[39;00m\n\u001b[0;32m    171\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalc_transition_matrices(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_set, X, inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: LinearChainCRFPytorch.viterbi() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# Train and test the PyTorch model\n",
    "torch_model = LinearChainCRFPytorch()\n",
    "torch_model.train('data/train.txt')\n",
    "torch_model.test('data/test.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "preprocess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
