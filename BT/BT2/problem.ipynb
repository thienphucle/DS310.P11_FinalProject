{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkTGHjwuJBMy",
        "outputId": "0e69e0a7-118f-42e0-bfb8-e247dd344d79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Tuple, Dict\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure necessary NLTK tokenizer models are available\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenize_type: str = \"basic\", lowercase: bool = False):\n",
        "        self.lowercase = lowercase\n",
        "        self.type = tokenize_type\n",
        "        self.vocab = []  # Empty vocabulary list\n",
        "\n",
        "    def basicTokenize(self, string: str) -> List[str]:\n",
        "        # Tokenizes input string by splitting on whitespace\n",
        "        ### BEGIN SOLUTION\n",
        "        return string.split()\n",
        "        ### END SOLUTION\n",
        "\n",
        "    def nltkTokenize(self, string: str) -> List[str]:\n",
        "        # Tokenizes input string using NLTK's word tokenizer\n",
        "        ### BEGIN SOLUTION\n",
        "        return word_tokenize(string)\n",
        "        ### END SOLUTION\n",
        "\n",
        "    def tokenize(self, string: str) -> List[str]:\n",
        "        # Tokenizes string and updates vocabulary with unique words\n",
        "        if self.lowercase:\n",
        "            string = string.lower()\n",
        "        tokens = self.basicTokenize(string) if self.type == \"basic\" else self.nltkTokenize(string)\n",
        "        self.vocab += [w for w in set(tokens) if w not in self.vocab]\n",
        "        return tokens\n",
        "\n",
        "    def countTopWords(self, words: List[str], k: int) -> List[Tuple[str, int]]:\n",
        "        # Returns the top k most common words\n",
        "        ### END SOLUTION\n",
        "        word_counts = Counter(words)\n",
        "        return word_counts.most_common(k)\n",
        "        ### END SOLUTION\n",
        "\n",
        "class BiGramLanguageModel:\n",
        "    def __init__(self, vocab: List[str], smoothing: str = None, smoothing_param: float = None):\n",
        "        self.vocab = vocab\n",
        "        self.token_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "        self.smoothing = smoothing\n",
        "        self.smoothing_param = smoothing_param\n",
        "        self.bi_counts = None\n",
        "        self.bi_prob = None\n",
        "        assert smoothing is None or smoothing_param is not None, \"Smoothing parameters must be set correctly.\"\n",
        "\n",
        "    def computeBigramProb(self):\n",
        "        # Computes bigram probabilities without smoothing\n",
        "        self.bi_prob = self.bi_counts.copy()\n",
        "        for i in range(len(self.bi_prob)):\n",
        "            cnt = np.sum(self.bi_prob[i])\n",
        "            if cnt > 0:\n",
        "                self.bi_prob[i] /= cnt\n",
        "\n",
        "    def computeBigramProbAddAlpha(self, alpha: float = 0.001):\n",
        "        # Computes bigram probabilities using add-alpha smoothing\n",
        "        ### BEGIN SOLUTION\n",
        "        self.bi_prob = self.bi_counts.copy() + alpha\n",
        "        row_sums = self.bi_prob.sum(axis=1, keepdims=True)\n",
        "        self.bi_prob /= row_sums\n",
        "        ### END SOLUTION\n",
        "\n",
        "    def train(self, corpus: List[str]):\n",
        "        # Trains the model on the given corpus\n",
        "        self.bi_counts = np.zeros((len(self.vocab), len(self.vocab)), dtype=float)\n",
        "        corpus_indices = [self.token_to_idx[w] for w in corpus]\n",
        "        for i in range(len(corpus_indices) - 1):\n",
        "            self.bi_counts[corpus_indices[i]][corpus_indices[i + 1]] += 1\n",
        "        if self.smoothing == \"addAlpha\":\n",
        "            self.computeBigramProbAddAlpha(self.smoothing_param)\n",
        "        else:\n",
        "            self.computeBigramProb()\n",
        "\n",
        "    def test(self, corpus: List[str]) -> float:\n",
        "        # Calculates and returns the perplexity of the model on the given corpus\n",
        "        logprob = 0.0\n",
        "        corpus_indices = [self.token_to_idx[w] for w in corpus]\n",
        "        for i in range(len(corpus_indices) - 1):\n",
        "            logprob += np.log(self.bi_prob[corpus_indices[i], corpus_indices[i + 1]])\n",
        "        logprob /= len(corpus_indices) - 1\n",
        "        return np.exp(-logprob)\n",
        "\n",
        "def readCorpus(filename: str, tokenizer: Tokenizer) -> List[str]:\n",
        "    # Reads and tokenizes the corpus from a file\n",
        "    ### BEGIN SOLUTION\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "      corpus = f.read()\n",
        "    return tokenizer.tokenize(corpus)\n",
        "    ### END SOLUTION\n",
        "\n",
        "def runLanguageModel(train_corpus: List[str], val_corpus: List[str], tokenizer: Tokenizer, smoothing_type: str = None, smoothing_param: float = 0.0) -> Dict[str, float]:\n",
        "    # Trains and tests the language model, returning key metrics\n",
        "    lm = BiGramLanguageModel(tokenizer.vocab, smoothing=smoothing_type, smoothing_param=smoothing_param)\n",
        "    lm.train(train_corpus)\n",
        "    return {\"train_ppl\": lm.test(train_corpus), \"val_ppl\": lm.test(val_corpus)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hnG3_6_dJ13u"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizers with basic and NLTK options, both set to lowercase.\n",
        "basic_tokenizer = Tokenizer(tokenize_type='basic', lowercase=True)\n",
        "nltk_tokenizer = Tokenizer(tokenize_type='nltk', lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7iqgIRBmJ4M2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "58f5fb6a-5385-47d0-804f-c0cfbf79aa4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './data/train.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-688c6ddf3d9b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read and tokenize the training and validation corpora using the basic tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/train.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mval_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/val.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example of using the NLTK tokenizer for comparison (unused in final results).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-30f3138ce80b>\u001b[0m in \u001b[0;36mreadCorpus\u001b[0;34m(filename, tokenizer)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Reads and tokenizes the corpus from a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m### BEGIN SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train.txt'"
          ]
        }
      ],
      "source": [
        "# Read and tokenize the training and validation corpora using the basic tokenizer.\n",
        "train_corpus = readCorpus('./data/train.txt', basic_tokenizer)\n",
        "val_corpus = readCorpus('./data/val.txt', basic_tokenizer)\n",
        "\n",
        "# Example of using the NLTK tokenizer for comparison (unused in final results).\n",
        "train_corpus_nltk = readCorpus('./data/train.txt', nltk_tokenizer)\n",
        "val_corpus_nltk = readCorpus('./data/val.txt', nltk_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev9y93YKL3ZH"
      },
      "outputs": [],
      "source": [
        "# Get top 10 frequent words and counts from train_corpus with basic_tokenizer.\n",
        "basic_tokenizer.countTopWords(train_corpus, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_199zQ3Rn7Q"
      },
      "outputs": [],
      "source": [
        "# Get top 10 frequent words and counts from train_corpus_nltk with nltk_tokenizer.\n",
        "nltk_tokenizer.countTopWords(train_corpus_nltk, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXk6-XB5KAPS"
      },
      "outputs": [],
      "source": [
        "# Run the language model with the basic tokenizer and without smoothing.\n",
        "runLanguageModel(train_corpus, val_corpus,\n",
        "                 tokenizer=basic_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oqKIFUwLF83"
      },
      "outputs": [],
      "source": [
        "# Run the language model with the nltk tokenizer and without smoothing.\n",
        "runLanguageModel(train_corpus_nltk, val_corpus_nltk,\n",
        "                 tokenizer=nltk_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7nLCm62KGh0"
      },
      "outputs": [],
      "source": [
        "# Run the language model with the basic tokenizer and with smoothing.\n",
        "runLanguageModel(train_corpus, val_corpus,\n",
        "                 tokenizer=basic_tokenizer, smoothing_type='addAlpha', smoothing_param=10e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05AvBa_sK3-b"
      },
      "outputs": [],
      "source": [
        "# Run the language model with the nltk tokenizer and with smoothing.\n",
        "runLanguageModel(train_corpus_nltk, val_corpus_nltk,\n",
        "                 tokenizer=nltk_tokenizer, smoothing_type='addAlpha', smoothing_param=10e-5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}